# NN_Bank_Churn_Prediction
Introduction to Neural Networks
Neural Network based classifier that can determine whether a customer will leave the bank or not in the next 6 months.
EDA
Data preprocessing
Model 1:

RMSprop: adjusts the learning rates for different parameters during training

ReLU Activation: enabling the model to learn complex relationships and patterns in the data.

Sigmoid: commonly used as an activation function in the output layer of a binary classification neural network.

Binary Crossentropy Loss Function: well-suited for training models in binary classification tasks, where the goal is to predict whether an example belongs to one of two classes.

Metric=Accuracy: a straightforward metric that measures the overall correctness of the model's predictions across both positive and negative classes.
Model 2:
Adam: helping the model learn efficiently and converge faster during training by adapting the step sizes based on how fast and in what direction the model's parameters are changing.

ReLU Activation: enabling the model to learn complex relationships and patterns in the data.

Sigmoid: commonly used as an activation function in the output layer of a binary classification neural network.

Binary Crossentropy Loss Function: well-suited for training models in binary classification tasks, where the goal is to predict whether an example belongs to one of two classes.

Metric=Accuracy: a straightforward metric that measures the overall correctness of the model's predictions across both positive and negative classes.
Model 3:
Adam: helping the model learn efficiently and converge faster during training by adapting the step sizes based on how fast and in what direction the model's parameters are changing.

ReLU Activation: enabling the model to learn complex relationships and patterns in the data.

Sigmoid: commonly used as an activation function in the output layer of a binary classification neural network.

Binary Crossentropy Loss Function: well-suited for training models in binary classification tasks, where the goal is to predict whether an example belongs to one of two classes.

Metric=Accuracy: a straightforward metric that measures the overall correctness of the model's predictions across both positive and negative classes.

Callbacks (early stopping): decides to stop the training process if it sees that the improvement in the validation loss is not significant.

Dropout: preventing the model from relying too much on specific features and improving robustness.
Model 4:
Adam: helping the model learn efficiently and converge faster during training by adapting the step sizes based on how fast and in what direction the model's parameters are changing.

ReLU Activation: enabling the model to learn complex relationships and patterns in the data.

Sigmoid: commonly used as an activation function in the output layer of a binary classification neural network.

Binary Crossentropy Loss Function: well-suited for training models in binary classification tasks, where the goal is to predict whether an example belongs to one of two classes.

Metric= Accuracy: a straightforward metric that measures the overall correctness of the model's predictions across both positive and negative classes.

Callbacks (early stopping): decides to stop the training process if it sees that the improvement in the validation loss is not significant.

Dropout: preventing the model from relying too much on specific features and improving robustness.
Model 5:
SMOTE: a technique used to address class imbalance by generating synthetic examples of the minority class.

Adam: helping the model learn efficiently and converge faster during training by adapting the step sizes based on how fast and in what direction the model's parameters are changing.

ReLU Activation: enabling the model to learn complex relationships and patterns in the data.

Sigmoid: commonly used as an activation function in the output layer of a binary classification neural network.

Binary Crossentropy Loss Function: well-suited for training models in binary classification tasks, where the goal is to predict whether an example belongs to one of two classes.

Metric=Accuracy: a straightforward metric that measures the overall correctness of the model's predictions across both positive and negative classes.

Dropout: preventing the model from relying too much on specific features and improving robustness.
